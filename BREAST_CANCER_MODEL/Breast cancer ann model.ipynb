{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Users\\Praty\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Praty\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Praty\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\Praty\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Praty\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Praty\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Praty\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Praty\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.models import load_model\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision = 3, suppress = True)\n",
    "from sklearn import model_selection\n",
    "\n",
    "#from tensorflow.keras import  layers\n",
    "#from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"C:/Users/Praty/OneDrive/Desktop/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.filter([\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\"compactness_mean\",\"concavity_mean\",\"concave points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\"radius_se\",\"texture_se\",\"perimeter_se\",\"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\"concave points_se\",\"symmetry_se\",\"fractal_dimension_se\",\"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\"concavity_worst\",\"concave points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\",\"diagnosis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.replace(\"M\",0, inplace = True)\n",
    "train_dataset.replace(\"B\",1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      "radius_mean                569 non-null float64\n",
      "texture_mean               569 non-null float64\n",
      "perimeter_mean             569 non-null float64\n",
      "area_mean                  569 non-null float64\n",
      "smoothness_mean            569 non-null float64\n",
      "compactness_mean           569 non-null float64\n",
      "concavity_mean             569 non-null float64\n",
      "concave points_mean        569 non-null float64\n",
      "symmetry_mean              569 non-null float64\n",
      "fractal_dimension_mean     569 non-null float64\n",
      "radius_se                  569 non-null float64\n",
      "texture_se                 569 non-null float64\n",
      "perimeter_se               569 non-null float64\n",
      "area_se                    569 non-null float64\n",
      "smoothness_se              569 non-null float64\n",
      "compactness_se             569 non-null float64\n",
      "concavity_se               569 non-null float64\n",
      "concave points_se          569 non-null float64\n",
      "symmetry_se                569 non-null float64\n",
      "fractal_dimension_se       569 non-null float64\n",
      "radius_worst               569 non-null float64\n",
      "texture_worst              569 non-null float64\n",
      "perimeter_worst            569 non-null float64\n",
      "area_worst                 569 non-null float64\n",
      "smoothness_worst           569 non-null float64\n",
      "compactness_worst          569 non-null float64\n",
      "concavity_worst            569 non-null float64\n",
      "concave points_worst       569 non-null float64\n",
      "symmetry_worst             569 non-null float64\n",
      "fractal_dimension_worst    569 non-null float64\n",
      "diagnosis                  569 non-null int64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['split'] = np.random.randn(train_dataset.shape[0], 1)\n",
    "\n",
    "msk = np.random.rand(len(train_dataset)) <= 0.7\n",
    "\n",
    "train = train_dataset[msk]\n",
    "test = train_dataset[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Dense(units = 96, input_shape=(30,), activation = 'relu'),\n",
    "        Dense(units = 64, activation='relu'),\n",
    "        Dense(units = 32, activation='relu'),\n",
    "        Dense(units = 24, activation='relu'),\n",
    "        Dense(units = 12, activation ='relu'),\n",
    "        Dense(units = 2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 96)                2976      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 24)                792       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 12,382\n",
      "Trainable params: 12,382\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list = train_dataset.columns.values\n",
    "df = train_dataset[header_list[0:31]]\n",
    "X = df.iloc[:,:30]\n",
    "Y = train_dataset[['diagnosis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     fractal_dimension_mean  ...  texture_worst  perimeter_worst  area_worst  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  diagnosis  \n",
       "0                  0.2654          0.4601                  0.11890          0  \n",
       "1                  0.1860          0.2750                  0.08902          0  \n",
       "2                  0.2430          0.3613                  0.08758          0  \n",
       "3                  0.2575          0.6638                  0.17300          0  \n",
       "4                  0.1625          0.2364                  0.07678          0  \n",
       "..                    ...             ...                      ...        ...  \n",
       "564                0.2216          0.2060                  0.07115          0  \n",
       "565                0.1628          0.2572                  0.06637          0  \n",
       "566                0.1418          0.2218                  0.07820          0  \n",
       "567                0.2650          0.4087                  0.12400          0  \n",
       "568                0.0000          0.2871                  0.07039          1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "57/57 - 0s - loss: 0.1370 - accuracy: 0.9525\n",
      "Epoch 2/120\n",
      "57/57 - 0s - loss: 0.1347 - accuracy: 0.9455\n",
      "Epoch 3/120\n",
      "57/57 - 0s - loss: 0.1431 - accuracy: 0.9385\n",
      "Epoch 4/120\n",
      "57/57 - 0s - loss: 0.1424 - accuracy: 0.9385\n",
      "Epoch 5/120\n",
      "57/57 - 0s - loss: 0.1233 - accuracy: 0.9543\n",
      "Epoch 6/120\n",
      "57/57 - 0s - loss: 0.1264 - accuracy: 0.9508\n",
      "Epoch 7/120\n",
      "57/57 - 0s - loss: 0.1189 - accuracy: 0.9473\n",
      "Epoch 8/120\n",
      "57/57 - 0s - loss: 0.1779 - accuracy: 0.9209\n",
      "Epoch 9/120\n",
      "57/57 - 0s - loss: 0.1204 - accuracy: 0.9508\n",
      "Epoch 10/120\n",
      "57/57 - 0s - loss: 0.1254 - accuracy: 0.9455\n",
      "Epoch 11/120\n",
      "57/57 - 0s - loss: 0.1459 - accuracy: 0.9420\n",
      "Epoch 12/120\n",
      "57/57 - 0s - loss: 0.1402 - accuracy: 0.9385\n",
      "Epoch 13/120\n",
      "57/57 - 0s - loss: 0.1177 - accuracy: 0.9508\n",
      "Epoch 14/120\n",
      "57/57 - 0s - loss: 0.1543 - accuracy: 0.9438\n",
      "Epoch 15/120\n",
      "57/57 - 0s - loss: 0.1464 - accuracy: 0.9420\n",
      "Epoch 16/120\n",
      "57/57 - 0s - loss: 0.1284 - accuracy: 0.9473\n",
      "Epoch 17/120\n",
      "57/57 - 0s - loss: 0.1320 - accuracy: 0.9490\n",
      "Epoch 18/120\n",
      "57/57 - 0s - loss: 0.1318 - accuracy: 0.9438\n",
      "Epoch 19/120\n",
      "57/57 - 0s - loss: 0.1249 - accuracy: 0.9561\n",
      "Epoch 20/120\n",
      "57/57 - 0s - loss: 0.1261 - accuracy: 0.9473\n",
      "Epoch 21/120\n",
      "57/57 - 0s - loss: 0.1193 - accuracy: 0.9420\n",
      "Epoch 22/120\n",
      "57/57 - 0s - loss: 0.1209 - accuracy: 0.9490\n",
      "Epoch 23/120\n",
      "57/57 - 0s - loss: 0.1446 - accuracy: 0.9315\n",
      "Epoch 24/120\n",
      "57/57 - 0s - loss: 0.1419 - accuracy: 0.9455\n",
      "Epoch 25/120\n",
      "57/57 - 0s - loss: 0.1337 - accuracy: 0.9438\n",
      "Epoch 26/120\n",
      "57/57 - 0s - loss: 0.1405 - accuracy: 0.9402\n",
      "Epoch 27/120\n",
      "57/57 - 0s - loss: 0.1132 - accuracy: 0.9508\n",
      "Epoch 28/120\n",
      "57/57 - 0s - loss: 0.1141 - accuracy: 0.9525\n",
      "Epoch 29/120\n",
      "57/57 - 0s - loss: 0.1404 - accuracy: 0.9490\n",
      "Epoch 30/120\n",
      "57/57 - 0s - loss: 0.1182 - accuracy: 0.9525\n",
      "Epoch 31/120\n",
      "57/57 - 0s - loss: 0.1132 - accuracy: 0.9543\n",
      "Epoch 32/120\n",
      "57/57 - 0s - loss: 0.1180 - accuracy: 0.9490\n",
      "Epoch 33/120\n",
      "57/57 - 0s - loss: 0.1239 - accuracy: 0.9508\n",
      "Epoch 34/120\n",
      "57/57 - 0s - loss: 0.1105 - accuracy: 0.9578\n",
      "Epoch 35/120\n",
      "57/57 - 0s - loss: 0.1960 - accuracy: 0.9279\n",
      "Epoch 36/120\n",
      "57/57 - 0s - loss: 0.1420 - accuracy: 0.9402\n",
      "Epoch 37/120\n",
      "57/57 - 0s - loss: 0.1342 - accuracy: 0.9473\n",
      "Epoch 38/120\n",
      "57/57 - 0s - loss: 0.1224 - accuracy: 0.9561\n",
      "Epoch 39/120\n",
      "57/57 - 0s - loss: 0.1907 - accuracy: 0.9244\n",
      "Epoch 40/120\n",
      "57/57 - 0s - loss: 0.1139 - accuracy: 0.9525\n",
      "Epoch 41/120\n",
      "57/57 - 0s - loss: 0.1108 - accuracy: 0.9508\n",
      "Epoch 42/120\n",
      "57/57 - 0s - loss: 0.1078 - accuracy: 0.9508\n",
      "Epoch 43/120\n",
      "57/57 - 0s - loss: 0.1200 - accuracy: 0.9473\n",
      "Epoch 44/120\n",
      "57/57 - 0s - loss: 0.1123 - accuracy: 0.9543\n",
      "Epoch 45/120\n",
      "57/57 - 0s - loss: 0.1099 - accuracy: 0.9525\n",
      "Epoch 46/120\n",
      "57/57 - 0s - loss: 0.1627 - accuracy: 0.9297\n",
      "Epoch 47/120\n",
      "57/57 - 0s - loss: 0.1885 - accuracy: 0.9156\n",
      "Epoch 48/120\n",
      "57/57 - 0s - loss: 0.1103 - accuracy: 0.9525\n",
      "Epoch 49/120\n",
      "57/57 - 0s - loss: 0.1060 - accuracy: 0.9561\n",
      "Epoch 50/120\n",
      "57/57 - 0s - loss: 0.1165 - accuracy: 0.9438\n",
      "Epoch 51/120\n",
      "57/57 - 0s - loss: 0.1191 - accuracy: 0.9543\n",
      "Epoch 52/120\n",
      "57/57 - 0s - loss: 0.1314 - accuracy: 0.9508\n",
      "Epoch 53/120\n",
      "57/57 - 0s - loss: 0.1113 - accuracy: 0.9543\n",
      "Epoch 54/120\n",
      "57/57 - 0s - loss: 0.1355 - accuracy: 0.9490\n",
      "Epoch 55/120\n",
      "57/57 - 0s - loss: 0.1157 - accuracy: 0.9508\n",
      "Epoch 56/120\n",
      "57/57 - 0s - loss: 0.1178 - accuracy: 0.9508\n",
      "Epoch 57/120\n",
      "57/57 - 0s - loss: 0.1092 - accuracy: 0.9561\n",
      "Epoch 58/120\n",
      "57/57 - 0s - loss: 0.1518 - accuracy: 0.9385\n",
      "Epoch 59/120\n",
      "57/57 - 0s - loss: 0.1228 - accuracy: 0.9508\n",
      "Epoch 60/120\n",
      "57/57 - 0s - loss: 0.1092 - accuracy: 0.9578\n",
      "Epoch 61/120\n",
      "57/57 - 0s - loss: 0.1222 - accuracy: 0.9561\n",
      "Epoch 62/120\n",
      "57/57 - 0s - loss: 0.1147 - accuracy: 0.9525\n",
      "Epoch 63/120\n",
      "57/57 - 0s - loss: 0.1052 - accuracy: 0.9613\n",
      "Epoch 64/120\n",
      "57/57 - 0s - loss: 0.1221 - accuracy: 0.9543\n",
      "Epoch 65/120\n",
      "57/57 - 0s - loss: 0.1182 - accuracy: 0.9402\n",
      "Epoch 66/120\n",
      "57/57 - 0s - loss: 0.2035 - accuracy: 0.9104\n",
      "Epoch 67/120\n",
      "57/57 - 0s - loss: 0.1591 - accuracy: 0.9350\n",
      "Epoch 68/120\n",
      "57/57 - 0s - loss: 0.1402 - accuracy: 0.9455\n",
      "Epoch 69/120\n",
      "57/57 - 0s - loss: 0.1148 - accuracy: 0.9473\n",
      "Epoch 70/120\n",
      "57/57 - 0s - loss: 0.1156 - accuracy: 0.9543\n",
      "Epoch 71/120\n",
      "57/57 - 0s - loss: 0.1053 - accuracy: 0.9649\n",
      "Epoch 72/120\n",
      "57/57 - 0s - loss: 0.1044 - accuracy: 0.9578\n",
      "Epoch 73/120\n",
      "57/57 - 0s - loss: 0.1145 - accuracy: 0.9490\n",
      "Epoch 74/120\n",
      "57/57 - 0s - loss: 0.1185 - accuracy: 0.9473\n",
      "Epoch 75/120\n",
      "57/57 - 0s - loss: 0.1107 - accuracy: 0.9561\n",
      "Epoch 76/120\n",
      "57/57 - 0s - loss: 0.1199 - accuracy: 0.9508\n",
      "Epoch 77/120\n",
      "57/57 - 0s - loss: 0.0955 - accuracy: 0.9490\n",
      "Epoch 78/120\n",
      "57/57 - 0s - loss: 0.1171 - accuracy: 0.9543\n",
      "Epoch 79/120\n",
      "57/57 - 0s - loss: 0.1139 - accuracy: 0.9525\n",
      "Epoch 80/120\n",
      "57/57 - 0s - loss: 0.1148 - accuracy: 0.9490\n",
      "Epoch 81/120\n",
      "57/57 - 0s - loss: 0.1949 - accuracy: 0.9244\n",
      "Epoch 82/120\n",
      "57/57 - 0s - loss: 0.1137 - accuracy: 0.9525\n",
      "Epoch 83/120\n",
      "57/57 - 0s - loss: 0.1090 - accuracy: 0.9596\n",
      "Epoch 84/120\n",
      "57/57 - 0s - loss: 0.1209 - accuracy: 0.9525\n",
      "Epoch 85/120\n",
      "57/57 - 0s - loss: 0.1227 - accuracy: 0.9455\n",
      "Epoch 86/120\n",
      "57/57 - 0s - loss: 0.1279 - accuracy: 0.9473\n",
      "Epoch 87/120\n",
      "57/57 - 0s - loss: 0.1093 - accuracy: 0.9508\n",
      "Epoch 88/120\n",
      "57/57 - 0s - loss: 0.1025 - accuracy: 0.9561\n",
      "Epoch 89/120\n",
      "57/57 - 0s - loss: 0.1134 - accuracy: 0.9561\n",
      "Epoch 90/120\n",
      "57/57 - 0s - loss: 0.1223 - accuracy: 0.9508\n",
      "Epoch 91/120\n",
      "57/57 - 0s - loss: 0.1176 - accuracy: 0.9490\n",
      "Epoch 92/120\n",
      "57/57 - 0s - loss: 0.1074 - accuracy: 0.9525\n",
      "Epoch 93/120\n",
      "57/57 - 0s - loss: 0.1400 - accuracy: 0.9455\n",
      "Epoch 94/120\n",
      "57/57 - 0s - loss: 0.1117 - accuracy: 0.9561\n",
      "Epoch 95/120\n",
      "57/57 - 0s - loss: 0.0993 - accuracy: 0.9631\n",
      "Epoch 96/120\n",
      "57/57 - 0s - loss: 0.1121 - accuracy: 0.9455\n",
      "Epoch 97/120\n",
      "57/57 - 0s - loss: 0.1287 - accuracy: 0.9455\n",
      "Epoch 98/120\n",
      "57/57 - 0s - loss: 0.0930 - accuracy: 0.9543\n",
      "Epoch 99/120\n",
      "57/57 - 0s - loss: 0.1138 - accuracy: 0.9578\n",
      "Epoch 100/120\n",
      "57/57 - 0s - loss: 0.1015 - accuracy: 0.9561\n",
      "Epoch 101/120\n",
      "57/57 - 0s - loss: 0.1060 - accuracy: 0.9525\n",
      "Epoch 102/120\n",
      "57/57 - 0s - loss: 0.1098 - accuracy: 0.9490\n",
      "Epoch 103/120\n",
      "57/57 - 0s - loss: 0.1064 - accuracy: 0.9543\n",
      "Epoch 104/120\n",
      "57/57 - 0s - loss: 0.0955 - accuracy: 0.9561\n",
      "Epoch 105/120\n",
      "57/57 - 0s - loss: 0.1169 - accuracy: 0.9473\n",
      "Epoch 106/120\n",
      "57/57 - 0s - loss: 0.1030 - accuracy: 0.9525\n",
      "Epoch 107/120\n",
      "57/57 - 0s - loss: 0.0920 - accuracy: 0.9596\n",
      "Epoch 108/120\n",
      "57/57 - 0s - loss: 0.0958 - accuracy: 0.9578\n",
      "Epoch 109/120\n",
      "57/57 - 0s - loss: 0.1316 - accuracy: 0.9420\n",
      "Epoch 110/120\n",
      "57/57 - 0s - loss: 0.0978 - accuracy: 0.9596\n",
      "Epoch 111/120\n",
      "57/57 - 0s - loss: 0.1048 - accuracy: 0.9596\n",
      "Epoch 112/120\n",
      "57/57 - 0s - loss: 0.0916 - accuracy: 0.9543\n",
      "Epoch 113/120\n",
      "57/57 - 0s - loss: 0.1121 - accuracy: 0.9596\n",
      "Epoch 114/120\n",
      "57/57 - 0s - loss: 0.1240 - accuracy: 0.9490\n",
      "Epoch 115/120\n",
      "57/57 - 0s - loss: 0.0969 - accuracy: 0.9596\n",
      "Epoch 116/120\n",
      "57/57 - 0s - loss: 0.1196 - accuracy: 0.9473\n",
      "Epoch 117/120\n",
      "57/57 - 0s - loss: 0.0987 - accuracy: 0.9578\n",
      "Epoch 118/120\n",
      "57/57 - 0s - loss: 0.0921 - accuracy: 0.9596\n",
      "Epoch 119/120\n",
      "57/57 - 0s - loss: 0.0903 - accuracy: 0.9543\n",
      "Epoch 120/120\n",
      "57/57 - 0s - loss: 0.1695 - accuracy: 0.9350\n"
     ]
    }
   ],
   "source": [
    "cl =model.fit(x = X, y = Y, batch_size = 10, epochs = 120, verbose =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df=(df-df.min())/(df.max()-df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n"
     ]
    }
   ],
   "source": [
    "properties = list(normalized_df.columns.values)\n",
    "properties.remove('diagnosis')\n",
    "#properties.remove('0')\n",
    "#properties.remove('1')\n",
    "print(properties)\n",
    "X = normalized_df[properties]\n",
    "y = normalized_df['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = model_selection.train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 1s 9ms/step - loss: 0.6701 - accuracy: 0.4628 - val_loss: 0.5803 - val_accuracy: 0.6923\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5535 - accuracy: 0.8277 - val_loss: 0.4407 - val_accuracy: 0.8571\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.9147 - val_loss: 0.3289 - val_accuracy: 0.9341\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3223 - accuracy: 0.9535 - val_loss: 0.2323 - val_accuracy: 0.9451\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2022 - accuracy: 0.9525 - val_loss: 0.1227 - val_accuracy: 0.9560\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9620 - val_loss: 0.3003 - val_accuracy: 0.8791\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9137 - val_loss: 0.1240 - val_accuracy: 0.9451\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1545 - accuracy: 0.9274 - val_loss: 0.0766 - val_accuracy: 0.9670\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0802 - accuracy: 0.9726 - val_loss: 0.0641 - val_accuracy: 0.9560\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0872 - accuracy: 0.9729 - val_loss: 0.0651 - val_accuracy: 0.9890\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0976 - accuracy: 0.9666 - val_loss: 0.0661 - val_accuracy: 0.9670\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0717 - accuracy: 0.9768 - val_loss: 0.0594 - val_accuracy: 0.9890\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0898 - accuracy: 0.9774 - val_loss: 0.0586 - val_accuracy: 0.9670\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0660 - accuracy: 0.9717 - val_loss: 0.0683 - val_accuracy: 0.9890\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0795 - accuracy: 0.9687 - val_loss: 0.0595 - val_accuracy: 0.9890\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1213 - accuracy: 0.9575 - val_loss: 0.0597 - val_accuracy: 0.9670\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 1.00 - 0s 2ms/step - loss: 0.1397 - accuracy: 0.9562 - val_loss: 0.0602 - val_accuracy: 0.9670\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0541 - accuracy: 0.9896 - val_loss: 0.0992 - val_accuracy: 0.9560\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0478 - accuracy: 0.9846 - val_loss: 0.0567 - val_accuracy: 0.9670\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9846 - val_loss: 0.0618 - val_accuracy: 0.9890\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9720 - val_loss: 0.0625 - val_accuracy: 0.9890\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1137 - accuracy: 0.9589 - val_loss: 0.0549 - val_accuracy: 0.9670\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9869 - val_loss: 0.0496 - val_accuracy: 0.9670\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0312 - accuracy: 0.9971 - val_loss: 0.0602 - val_accuracy: 0.9780\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 0.9893 - val_loss: 0.0476 - val_accuracy: 0.9780\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 0.9867 - val_loss: 0.1205 - val_accuracy: 0.9560\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1058 - accuracy: 0.9657 - val_loss: 0.0853 - val_accuracy: 0.9451\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9916 - val_loss: 0.1063 - val_accuracy: 0.9560\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0724 - accuracy: 0.9768 - val_loss: 0.0595 - val_accuracy: 0.9670\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0310 - accuracy: 0.9939 - val_loss: 0.0809 - val_accuracy: 0.9560\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.9953 - val_loss: 0.1532 - val_accuracy: 0.9560\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0703 - accuracy: 0.9669 - val_loss: 0.0484 - val_accuracy: 0.9670\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9875 - val_loss: 0.1030 - val_accuracy: 0.9560\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9903 - val_loss: 0.0473 - val_accuracy: 0.9890\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0439 - accuracy: 0.9802 - val_loss: 0.0814 - val_accuracy: 0.9560\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0835 - accuracy: 0.9751 - val_loss: 0.0825 - val_accuracy: 0.9780\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0986 - accuracy: 0.9563 - val_loss: 0.0744 - val_accuracy: 0.9670\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0447 - accuracy: 0.9920 - val_loss: 0.0573 - val_accuracy: 0.9670\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9906 - val_loss: 0.0922 - val_accuracy: 0.9560\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9897 - val_loss: 0.0929 - val_accuracy: 0.9560\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 0.9901 - val_loss: 0.0593 - val_accuracy: 0.9670\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0329 - accuracy: 0.9902 - val_loss: 0.0559 - val_accuracy: 0.9670\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0443 - accuracy: 0.9869 - val_loss: 0.0599 - val_accuracy: 0.9670\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.9895 - val_loss: 0.0820 - val_accuracy: 0.9560\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.90 - 0s 2ms/step - loss: 0.0377 - accuracy: 0.9829 - val_loss: 0.0664 - val_accuracy: 0.9670\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9843 - val_loss: 0.0494 - val_accuracy: 0.9670\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0395 - accuracy: 0.9875 - val_loss: 0.0523 - val_accuracy: 0.9890\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9810 - val_loss: 0.0612 - val_accuracy: 0.9670\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 0.9862 - val_loss: 0.1024 - val_accuracy: 0.9560\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0593 - accuracy: 0.9836 - val_loss: 0.0924 - val_accuracy: 0.9560\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0277 - accuracy: 0.9925 - val_loss: 0.0593 - val_accuracy: 0.9670\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9753 - val_loss: 0.0562 - val_accuracy: 0.9670\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0410 - accuracy: 0.9816 - val_loss: 0.0541 - val_accuracy: 0.9670\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9913 - val_loss: 0.0554 - val_accuracy: 0.9670\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.9942 - val_loss: 0.0938 - val_accuracy: 0.9560\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9752 - val_loss: 0.0508 - val_accuracy: 0.9780\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1178 - accuracy: 0.9477 - val_loss: 0.1071 - val_accuracy: 0.9560\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0443 - accuracy: 0.9857 - val_loss: 0.0506 - val_accuracy: 0.9670\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.9948 - val_loss: 0.1088 - val_accuracy: 0.9560\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0587 - accuracy: 0.9825 - val_loss: 0.0504 - val_accuracy: 0.9780\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0142 - accuracy: 0.9982 - val_loss: 0.0752 - val_accuracy: 0.9670\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 0.9894 - val_loss: 0.0530 - val_accuracy: 0.9670\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9977 - val_loss: 0.0509 - val_accuracy: 0.9670\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9987 - val_loss: 0.1341 - val_accuracy: 0.9560\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.9959 - val_loss: 0.0551 - val_accuracy: 0.9670\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9980 - val_loss: 0.0811 - val_accuracy: 0.9670\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.9946 - val_loss: 0.0616 - val_accuracy: 0.9890\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 1.00 - 0s 2ms/step - loss: 0.0647 - accuracy: 0.9774 - val_loss: 0.0750 - val_accuracy: 0.9670\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 0.9898 - val_loss: 0.0591 - val_accuracy: 0.9670\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.9937 - val_loss: 0.0616 - val_accuracy: 0.9890\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 0.9976 - val_loss: 0.0616 - val_accuracy: 0.9780\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 4.6021e-04 - accuracy: 1.00 - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9957 - val_loss: 0.0875 - val_accuracy: 0.9780\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.9880 - val_loss: 0.0622 - val_accuracy: 0.9670\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9943 - val_loss: 0.0886 - val_accuracy: 0.9560\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9719 - val_loss: 0.0569 - val_accuracy: 0.9780\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0369 - accuracy: 0.9876 - val_loss: 0.0613 - val_accuracy: 0.9670\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 0.9987 - val_loss: 0.0539 - val_accuracy: 0.9670\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.9911 - val_loss: 0.0564 - val_accuracy: 0.9670\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 0.9893 - val_loss: 0.0651 - val_accuracy: 0.9670\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9997 - val_loss: 0.0720 - val_accuracy: 0.9780\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.9792 - val_loss: 0.0505 - val_accuracy: 0.9890\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9976 - val_loss: 0.0586 - val_accuracy: 0.9780\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0543 - val_accuracy: 0.9890\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9981 - val_loss: 0.0584 - val_accuracy: 0.9890\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.0700 - val_accuracy: 0.9780\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9931 - val_loss: 0.0819 - val_accuracy: 0.9780\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.9907 - val_loss: 0.0951 - val_accuracy: 0.9560\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.9889 - val_loss: 0.0768 - val_accuracy: 0.9670\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9981 - val_loss: 0.0592 - val_accuracy: 0.9670\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.0664 - val_accuracy: 0.9780\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.9915 - val_loss: 0.0796 - val_accuracy: 0.9780\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.0699 - val_accuracy: 0.9670\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.0729 - val_accuracy: 0.9670\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9985 - val_loss: 0.0787 - val_accuracy: 0.9670\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0695 - val_accuracy: 0.9670\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9938 - val_loss: 0.2048 - val_accuracy: 0.9560\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1432 - accuracy: 0.9649 - val_loss: 0.0728 - val_accuracy: 0.9670\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 0.9918 - val_loss: 0.0765 - val_accuracy: 0.9670\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9932 - val_loss: 0.1494 - val_accuracy: 0.9560\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0372 - accuracy: 0.9888 - val_loss: 0.1001 - val_accuracy: 0.9670\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "#from keras.optimizers import SGD\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(30,)),\n",
    "    keras.layers.Dense(96, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(24, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(2, activation=tf.nn.sigmoid),\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size = 10, epochs = 100,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ce12b43308>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'],label=\"Loss\")\n",
    "plt.plot(history.history['val_loss'],label=\"Validation Loss\")\n",
    "plt.title(\"CNN Classifier Training Loss Curves\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISITC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, X_Test, Y_Train, Y_Test = model_selection.train_test_split(X, Y, test_size=0.20, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION : 0.9385964912280702\n",
      "ACCURACY OF MODEL : 0.9385964912280702\n",
      "CROSS-V: 0.9630957925787922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Praty\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\Praty\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "Logistic_Reg = LogisticRegression(penalty = 'l2', tol = 0.0001, C = 1.0)\n",
    "Logistic_Reg.fit(X_Train, Y_Train)\n",
    "print(\"LOGISTIC REGRESSION :\", Logistic_Reg.score(X_Test, Y_Test))\n",
    "\n",
    "Y_PREDICTION = Logistic_Reg.predict(X_Test)\n",
    "print(\"ACCURACY OF MODEL :\", metrics.accuracy_score(Y_Test, Y_PREDICTION))\n",
    "\n",
    "#USING CROSS VALIDATION\n",
    "\n",
    "scores = cross_val_score(Logistic_Reg, X, y, cv = 5)\n",
    "print(\"CROSS-V:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE MODELS H5 FILE FOR ANN and Pickle for LOGISTIC REG\n",
    "from keras.models import load_model\n",
    "model.save(\"C:/Users/Praty/OneDrive/Desktop/breast_cancer.h5\")\n",
    "filename = 'C:/Users/Praty/OneDrive/Desktop/Breast_cancer_lr.pckl' \n",
    "pickle.dump(Logistic_Reg, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
